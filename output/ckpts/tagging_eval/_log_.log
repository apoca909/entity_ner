2024-04-09 05:31:37,120 - INFO - __log__: taskdir ./output/ckpts/tagging_eval
2024-04-09 05:31:37,122 - INFO - __log__: Process rank:-1, device:cpu, n_gpu:0, dist training:False, 16-bits:False
2024-04-09 05:31:37,123 - INFO - __log__: Training/evaluation parameters Namespace(task='tagging', nline=-1, config_name='./raw_disf/bert_tagging.json', init_checkpoint='hfl/chinese-bert-wwm-ext', do_train=False, do_eval=False, input_format='raw', train_file=[''], dev_file=[''], save_mode='gold_pred', output_dir='./output/ckpts/', overwrite_outputdir=True, no_cuda=False, max_seq_length=512, warmup_steps=0, label_file='./raw_disf/label_map.json', vocab_file='./raw_disf/vocab.txt', predict_batch_size=32, num_output_variants=0, local_rank=-1, server_ip='', server_port='', max_steps=-1, gradient_accumulation_steps=1, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, logging_steps=1000, save_steps=50, per_gpu_train_batch_size=16, per_gpu_eval_batch_size=32, num_train_epochs=20, learning_rate=0.0002, evaluate_during_training=True, overwrite=True, fp16=False, fp16_opt_level='O1', seed=42, clsmodel='./output/model_cnn/model_19.bin', note='desc note', n_gpu=0, device=device(type='cpu'))
2024-04-09 05:32:02,812 - INFO - __log__: taskdir ./output/ckpts/tagging_eval
2024-04-09 05:32:02,812 - INFO - __log__: Process rank:-1, device:cpu, n_gpu:0, dist training:False, 16-bits:False
2024-04-09 05:32:02,813 - INFO - __log__: Training/evaluation parameters Namespace(task='tagging', nline=-1, config_name='./raw_disf/bert_tagging.json', init_checkpoint='hfl/chinese-bert-wwm-ext', do_train=False, do_eval=False, input_format='raw', train_file=[''], dev_file=[''], save_mode='gold_pred', output_dir='./output/ckpts/', overwrite_outputdir=True, no_cuda=False, max_seq_length=512, warmup_steps=0, label_file='./raw_disf/label_map.json', vocab_file='./raw_disf/vocab.txt', predict_batch_size=32, num_output_variants=0, local_rank=-1, server_ip='', server_port='', max_steps=-1, gradient_accumulation_steps=1, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, logging_steps=1000, save_steps=50, per_gpu_train_batch_size=16, per_gpu_eval_batch_size=32, num_train_epochs=20, learning_rate=0.0002, evaluate_during_training=True, overwrite=True, fp16=False, fp16_opt_level='O1', seed=42, clsmodel='./output/model_cnn/model_19.bin', note='desc note', n_gpu=0, device=device(type='cpu'))
2024-04-09 05:32:17,355 - INFO - __log__: taskdir ./output/ckpts/tagging_eval
2024-04-09 05:32:17,356 - INFO - __log__: Process rank:-1, device:cpu, n_gpu:0, dist training:False, 16-bits:False
2024-04-09 05:32:17,356 - INFO - __log__: Training/evaluation parameters Namespace(task='tagging', nline=-1, config_name='./raw_disf/bert_tagging.json', init_checkpoint='hfl/chinese-bert-wwm-ext', do_train=False, do_eval=False, input_format='raw', train_file=[''], dev_file=[''], save_mode='gold_pred', output_dir='./output/ckpts/', overwrite_outputdir=True, no_cuda=False, max_seq_length=512, warmup_steps=0, label_file='./raw_disf/label_map.json', vocab_file='./raw_disf/vocab.txt', predict_batch_size=32, num_output_variants=0, local_rank=-1, server_ip='', server_port='', max_steps=-1, gradient_accumulation_steps=1, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, logging_steps=1000, save_steps=50, per_gpu_train_batch_size=16, per_gpu_eval_batch_size=32, num_train_epochs=20, learning_rate=0.0002, evaluate_during_training=True, overwrite=True, fp16=False, fp16_opt_level='O1', seed=42, clsmodel='./output/model_cnn/model_19.bin', note='desc note', n_gpu=0, device=device(type='cpu'))
2024-04-09 05:32:53,868 - INFO - __log__: taskdir ./output/ckpts/tagging_eval
2024-04-09 05:32:53,869 - INFO - __log__: Process rank:-1, device:cpu, n_gpu:0, dist training:False, 16-bits:False
2024-04-09 05:32:53,869 - INFO - __log__: Training/evaluation parameters Namespace(task='tagging', nline=-1, config_name='./raw_disf/bert_tagging.json', init_checkpoint='hfl/chinese-bert-wwm-ext', do_train=False, do_eval=False, input_format='raw', train_file=[''], dev_file=[''], save_mode='gold_pred', output_dir='./output/ckpts/', overwrite_outputdir=True, no_cuda=False, max_seq_length=512, warmup_steps=0, label_file='./raw_disf/label_map.json', vocab_file='./raw_disf/vocab.txt', predict_batch_size=32, num_output_variants=0, local_rank=-1, server_ip='', server_port='', max_steps=-1, gradient_accumulation_steps=1, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, logging_steps=1000, save_steps=50, per_gpu_train_batch_size=16, per_gpu_eval_batch_size=32, num_train_epochs=20, learning_rate=0.0002, evaluate_during_training=True, overwrite=True, fp16=False, fp16_opt_level='O1', seed=42, clsmodel='./output/model_cnn/model_19.bin', note='desc note', n_gpu=0, device=device(type='cpu'))
2024-04-09 05:33:17,844 - INFO - __log__: taskdir ./output/ckpts/tagging_eval
2024-04-09 05:33:17,844 - INFO - __log__: Process rank:-1, device:cpu, n_gpu:0, dist training:False, 16-bits:False
2024-04-09 05:33:17,845 - INFO - __log__: Training/evaluation parameters Namespace(task='tagging', nline=-1, config_name='./raw_disf/bert_tagging.json', init_checkpoint='hfl/chinese-bert-wwm-ext', do_train=False, do_eval=False, input_format='raw', train_file=[''], dev_file=[''], save_mode='gold_pred', output_dir='./output/ckpts/', overwrite_outputdir=True, no_cuda=False, max_seq_length=512, warmup_steps=0, label_file='./raw_disf/label_map.json', vocab_file='./raw_disf/vocab.txt', predict_batch_size=32, num_output_variants=0, local_rank=-1, server_ip='', server_port='', max_steps=-1, gradient_accumulation_steps=1, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, logging_steps=1000, save_steps=50, per_gpu_train_batch_size=16, per_gpu_eval_batch_size=32, num_train_epochs=20, learning_rate=0.0002, evaluate_during_training=True, overwrite=True, fp16=False, fp16_opt_level='O1', seed=42, clsmodel='./output/model_cnn/model_19.bin', note='desc note', n_gpu=0, device=device(type='cpu'))
2024-04-09 05:33:59,913 - INFO - __log__: taskdir ./output/ckpts/tagging_eval
2024-04-09 05:33:59,914 - INFO - __log__: Process rank:-1, device:cpu, n_gpu:0, dist training:False, 16-bits:False
2024-04-09 05:33:59,915 - INFO - __log__: Training/evaluation parameters Namespace(task='tagging', nline=-1, config_name='./raw_disf/bert_tagging.json', init_checkpoint='hfl/chinese-bert-wwm-ext', do_train=False, do_eval=False, input_format='raw', train_file=[''], dev_file=[''], save_mode='gold_pred', output_dir='./output/ckpts/', overwrite_outputdir=True, no_cuda=False, max_seq_length=512, warmup_steps=0, label_file='./raw_disf/label_map.json', vocab_file='./raw_disf/vocab.txt', predict_batch_size=32, num_output_variants=0, local_rank=-1, server_ip='', server_port='', max_steps=-1, gradient_accumulation_steps=1, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, logging_steps=1000, save_steps=50, per_gpu_train_batch_size=16, per_gpu_eval_batch_size=32, num_train_epochs=20, learning_rate=0.0002, evaluate_during_training=True, overwrite=True, fp16=False, fp16_opt_level='O1', seed=42, clsmodel='./output/model_cnn/model_19.bin', note='desc note', n_gpu=0, device=device(type='cpu'))
2024-04-09 05:34:56,126 - INFO - __log__: taskdir ./output/ckpts/tagging_eval
2024-04-09 05:34:56,127 - INFO - __log__: Process rank:-1, device:cpu, n_gpu:0, dist training:False, 16-bits:False
2024-04-09 05:34:56,127 - INFO - __log__: Training/evaluation parameters Namespace(task='tagging', usecrf=True, nline=-1, config_name='./raw_disf/bert_tagging.json', init_checkpoint='hfl/chinese-bert-wwm-ext', do_train=False, do_eval=False, input_format='raw', train_file=[''], dev_file=[''], save_mode='gold_pred', output_dir='./output/ckpts/', overwrite_outputdir=True, no_cuda=False, max_seq_length=512, warmup_steps=0, label_file='./raw_disf/label_map.json', vocab_file='./raw_disf/vocab.txt', predict_batch_size=32, num_output_variants=0, local_rank=-1, server_ip='', server_port='', max_steps=-1, gradient_accumulation_steps=1, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, logging_steps=1000, save_steps=50, per_gpu_train_batch_size=16, per_gpu_eval_batch_size=32, num_train_epochs=20, learning_rate=0.0002, evaluate_during_training=True, overwrite=True, fp16=False, fp16_opt_level='O1', seed=42, clsmodel='./output/model_cnn/model_19.bin', note='desc note', n_gpu=0, device=device(type='cpu'))
2024-04-09 05:34:58,202 - INFO - __log__: ckpt loaded hfl/chinese-bert-wwm-ext done.
2024-04-09 05:34:58,203 - INFO - __log__: the number of model params: 102965556
2024-04-09 05:34:58,203 - INFO - __log__: Training/evaluation parameters Namespace(task='tagging', usecrf=True, nline=-1, config_name='./raw_disf/bert_tagging.json', init_checkpoint='hfl/chinese-bert-wwm-ext', do_train=False, do_eval=False, input_format='raw', train_file=[''], dev_file=[''], save_mode='gold_pred', output_dir='./output/ckpts/', overwrite_outputdir=True, no_cuda=False, max_seq_length=512, warmup_steps=0, label_file='./raw_disf/label_map.json', vocab_file='./raw_disf/vocab.txt', predict_batch_size=32, num_output_variants=0, local_rank=-1, server_ip='', server_port='', max_steps=-1, gradient_accumulation_steps=1, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, logging_steps=1000, save_steps=50, per_gpu_train_batch_size=16, per_gpu_eval_batch_size=32, num_train_epochs=20, learning_rate=0.0002, evaluate_during_training=True, overwrite=True, fp16=False, fp16_opt_level='O1', seed=42, clsmodel='./output/model_cnn/model_19.bin', note='desc note', n_gpu=0, device=device(type='cpu'))
2024-04-09 05:38:20,530 - INFO - __log__: taskdir ./output/ckpts/tagging_eval
2024-04-09 05:38:20,531 - INFO - __log__: Process rank:-1, device:cpu, n_gpu:0, dist training:False, 16-bits:False
2024-04-09 05:38:20,531 - INFO - __log__: Training/evaluation parameters Namespace(task='tagging', usecrf=True, nline=-1, config_name='./raw_disf/bert_tagging.json', init_checkpoint='hfl/chinese-bert-wwm-ext', do_train=False, do_eval=False, input_format='raw', train_file=[''], dev_file=[''], save_mode='gold_pred', output_dir='./output/ckpts/', overwrite_outputdir=True, no_cuda=False, max_seq_length=512, warmup_steps=0, label_file='./raw_disf/label_map.json', vocab_file='./raw_disf/vocab.txt', predict_batch_size=32, num_output_variants=0, local_rank=-1, server_ip='', server_port='', max_steps=-1, gradient_accumulation_steps=1, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, logging_steps=1000, save_steps=50, per_gpu_train_batch_size=16, per_gpu_eval_batch_size=32, num_train_epochs=20, learning_rate=0.0002, evaluate_during_training=True, overwrite=True, fp16=False, fp16_opt_level='O1', seed=42, clsmodel='./output/model_cnn/model_19.bin', note='desc note', n_gpu=0, device=device(type='cpu'))
2024-04-09 05:38:22,425 - INFO - __log__: ckpt loaded hfl/chinese-bert-wwm-ext done.
2024-04-09 05:38:22,426 - INFO - __log__: the number of model params: 102965556
2024-04-09 05:38:22,426 - INFO - __log__: Training/evaluation parameters Namespace(task='tagging', usecrf=True, nline=-1, config_name='./raw_disf/bert_tagging.json', init_checkpoint='hfl/chinese-bert-wwm-ext', do_train=False, do_eval=False, input_format='raw', train_file=[''], dev_file=[''], save_mode='gold_pred', output_dir='./output/ckpts/', overwrite_outputdir=True, no_cuda=False, max_seq_length=512, warmup_steps=0, label_file='./raw_disf/label_map.json', vocab_file='./raw_disf/vocab.txt', predict_batch_size=32, num_output_variants=0, local_rank=-1, server_ip='', server_port='', max_steps=-1, gradient_accumulation_steps=1, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, logging_steps=1000, save_steps=50, per_gpu_train_batch_size=16, per_gpu_eval_batch_size=32, num_train_epochs=20, learning_rate=0.0002, evaluate_during_training=True, overwrite=True, fp16=False, fp16_opt_level='O1', seed=42, clsmodel='./output/model_cnn/model_19.bin', note='desc note', n_gpu=0, device=device(type='cpu'))
2024-04-09 05:39:07,792 - INFO - __log__: taskdir ./output/ckpts/tagging_eval
2024-04-09 05:39:07,792 - INFO - __log__: Process rank:-1, device:cpu, n_gpu:0, dist training:False, 16-bits:False
2024-04-09 05:39:07,793 - INFO - __log__: Training/evaluation parameters Namespace(task='tagging', usecrf=True, nline=-1, config_name='./raw_disf/bert_tagging.json', init_checkpoint='hfl/chinese-bert-wwm-ext', do_train=False, do_eval=False, input_format='raw', train_file=[''], dev_file=[''], save_mode='gold_pred', output_dir='./output/ckpts/', overwrite_outputdir=True, no_cuda=False, max_seq_length=512, warmup_steps=0, label_file='./raw_disf/label_map.json', vocab_file='./raw_disf/vocab.txt', predict_batch_size=32, num_output_variants=0, local_rank=-1, server_ip='', server_port='', max_steps=-1, gradient_accumulation_steps=1, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, logging_steps=1000, save_steps=50, per_gpu_train_batch_size=16, per_gpu_eval_batch_size=32, num_train_epochs=20, learning_rate=0.0002, evaluate_during_training=True, overwrite=True, fp16=False, fp16_opt_level='O1', seed=42, clsmodel='./output/model_cnn/model_19.bin', note='desc note', n_gpu=0, device=device(type='cpu'))
2024-04-09 05:39:11,134 - INFO - __log__: ckpt loaded hfl/chinese-bert-wwm-ext done.
2024-04-09 05:39:11,136 - INFO - __log__: the number of model params: 102965556
2024-04-09 05:39:11,136 - INFO - __log__: Training/evaluation parameters Namespace(task='tagging', usecrf=True, nline=-1, config_name='./raw_disf/bert_tagging.json', init_checkpoint='hfl/chinese-bert-wwm-ext', do_train=False, do_eval=False, input_format='raw', train_file=[''], dev_file=[''], save_mode='gold_pred', output_dir='./output/ckpts/', overwrite_outputdir=True, no_cuda=False, max_seq_length=512, warmup_steps=0, label_file='./raw_disf/label_map.json', vocab_file='./raw_disf/vocab.txt', predict_batch_size=32, num_output_variants=0, local_rank=-1, server_ip='', server_port='', max_steps=-1, gradient_accumulation_steps=1, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, logging_steps=1000, save_steps=50, per_gpu_train_batch_size=16, per_gpu_eval_batch_size=32, num_train_epochs=20, learning_rate=0.0002, evaluate_during_training=True, overwrite=True, fp16=False, fp16_opt_level='O1', seed=42, clsmodel='./output/model_cnn/model_19.bin', note='desc note', n_gpu=0, device=device(type='cpu'))
2024-04-09 05:39:46,320 - INFO - __log__: taskdir ./output/ckpts/tagging_eval
2024-04-09 05:40:00,742 - INFO - __log__: taskdir ./output/ckpts/tagging_eval
2024-04-09 05:40:13,603 - INFO - __log__: Process rank:-1, device:cpu, n_gpu:0, dist training:False, 16-bits:False
2024-04-09 05:40:14,274 - INFO - __log__: Training/evaluation parameters Namespace(task='tagging', usecrf=True, nline=-1, config_name='./raw_disf/bert_tagging.json', init_checkpoint='hfl/chinese-bert-wwm-ext', do_train=False, do_eval=False, input_format='raw', train_file=[''], dev_file=[''], save_mode='gold_pred', output_dir='./output/ckpts/', overwrite_outputdir=True, no_cuda=False, max_seq_length=512, warmup_steps=0, label_file='./raw_disf/label_map.json', vocab_file='./raw_disf/vocab.txt', predict_batch_size=32, num_output_variants=0, local_rank=-1, server_ip='', server_port='', max_steps=-1, gradient_accumulation_steps=1, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, logging_steps=1000, save_steps=50, per_gpu_train_batch_size=16, per_gpu_eval_batch_size=32, num_train_epochs=20, learning_rate=0.0002, evaluate_during_training=True, overwrite=True, fp16=False, fp16_opt_level='O1', seed=42, clsmodel='./output/model_cnn/model_19.bin', note='desc note', n_gpu=0, device=device(type='cpu'))
2024-04-09 05:40:22,988 - INFO - __log__: ckpt loaded hfl/chinese-bert-wwm-ext done.
2024-04-09 05:40:26,689 - INFO - __log__: the number of model params: 102965556
